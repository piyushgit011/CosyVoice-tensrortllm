/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 01-16 08:57:55 [__init__.py:216] Automatically detected platform cuda.
ğŸš€ CosyVoice3 Advanced Benchmark - FP8 & Multi-Worker Tests
Testing advanced configurations for maximum performance

âš ï¸  NOTE: FP8 quantization support depends on:
   - GPU capability (RTX 5090 supports FP8)
   - vLLM version and configuration
   - Model architecture compatibility
   We'll test what's available in this setup.


================================================================================
Testing: vLLM FP32 (baseline)
  Quantization: fp32
  vLLM: True
================================================================================
Loading model...
2026-01-16 08:58:10,012 INFO input frame rate=25
/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
[1;31m2026-01-16 08:58:14.851360650 [E:onnxruntime:Default, provider_bridge_ort.cc:1744 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.11: cannot open shared object file: No such file or directory
[m
[0;93m2026-01-16 08:58:14.851382879 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:870 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.[m
2026-01-16 08:58:16,393 INFO use wetext frontend
INFO 01-16 08:58:26 [model.py:547] Resolved architecture: Qwen2ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-16 08:58:26 [model.py:1510] Using max model len 32768
WARNING 01-16 08:58:26 [arg_utils.py:1554] --enable-prompt-embeds and --enable-prefix-caching are not supported together in V1. Prefix caching has been disabled.
INFO 01-16 08:58:27 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 01-16 08:58:27 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 01-16 08:58:30 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:35 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='pretrained_models/Fun-CosyVoice3-0.5B/vllm', speculative_config=None, tokenizer='pretrained_models/Fun-CosyVoice3-0.5B/vllm', skip_tokenizer_init=True, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=pretrained_models/Fun-CosyVoice3-0.5B/vllm, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":256,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:35 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=25317)[0;0m WARNING 01-16 08:58:35 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:35 [gpu_model_runner.py:2602] Starting to load model pretrained_models/Fun-CosyVoice3-0.5B/vllm...
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:36 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:36 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=25317)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=25317)[0;0m 2026-01-16 08:58:37,385 DEBUG Starting new HTTPS connection (1): stats.vllm.ai:443
[1;36m(EngineCore_DP0 pid=25317)[0;0m 2026-01-16 08:58:37,486 DEBUG https://stats.vllm.ai:443 "POST / HTTP/1.1" 200 None
[1;36m(EngineCore_DP0 pid=25317)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.37s/it]
[1;36m(EngineCore_DP0 pid=25317)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.37s/it]
[1;36m(EngineCore_DP0 pid=25317)[0;0m 
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:37 [default_loader.py:267] Loading weights took 1.41 seconds
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:38 [gpu_model_runner.py:2653] Model loading took 0.6953 GiB and 1.561337 seconds
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:41 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/8801fce406/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:41 [backends.py:559] Dynamo bytecode transform time: 2.84 s
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:42 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.907 s
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:42 [monitor.py:34] torch.compile takes 2.84 s in total
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:43 [gpu_worker.py:298] Available KV cache memory: 5.42 GiB
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:43 [kv_cache_utils.py:1087] GPU KV cache size: 473,472 tokens
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 14.45x
[1;36m(EngineCore_DP0 pid=25317)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 6/35 [00:00<00:00, 57.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:00<00:00, 59.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:00<00:00, 63.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:00<00:00, 66.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 60.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 61.53it/s]
[1;36m(EngineCore_DP0 pid=25317)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆ         | 2/19 [00:00<00:00, 18.13it/s]Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 46.19it/s]Capturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:00<00:00, 55.99it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 52.71it/s]
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:45 [gpu_model_runner.py:3480] Graph capturing finished in 2 secs, took 0.32 GiB
[1;36m(EngineCore_DP0 pid=25317)[0;0m INFO 01-16 08:58:45 [core.py:210] init engine (profile, create kv cache, warmup model) took 7.19 seconds
INFO 01-16 08:58:45 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 29592
[01/16/2026-08:58:48] [TRT] [I] Loaded engine size: 1270 MiB
[01/16/2026-08:58:48] [TRT] [I] [MS] Running engine with multi stream info
[01/16/2026-08:58:48] [TRT] [I] [MS] Number of aux streams is 1
[01/16/2026-08:58:48] [TRT] [I] [MS] Number of total worker streams is 2
[01/16/2026-08:58:48] [TRT] [I] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[01/16/2026-08:58:48] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3552, now: CPU 0, GPU 4815 (MiB)
âœ… Model loaded in 48.62s
ğŸ“Š Memory: Allocated 654MB, Reserved 3482MB
Warming up...
  0%|          | 0/1 [00:00<?, ?it/s]2026-01-16 08:58:48,909 WARNING synthesis text ä½ å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³åˆæˆç³»ç»Ÿã€‚ too short than prompt text You are a helpful assistant.<|endofprompt|>å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚, this may lead to bad performance
/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.
  warnings.warn(
2026-01-16 08:58:49,742 INFO synthesis text ä½ å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³åˆæˆç³»ç»Ÿã€‚
/workspace/CosyVoice-tensrortllm/cosyvoice/cli/model.py:103: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with self.llm_context, torch.cuda.amp.autocast(self.fp16 is True and hasattr(self.llm, 'vllm') is False):
WARNING 01-16 08:58:49 [preprocess.py:60] Using None for EOS token id because tokenizer is not initialized
/workspace/CosyVoice-tensrortllm/cosyvoice/cli/model.py:417: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(self.fp16):
2026-01-16 08:58:50,584 INFO yield speech len 1.36, rtf 0.6194460041382733
2026-01-16 08:58:50,697 INFO yield speech len 1.72, rtf 0.06543300872625307
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.81s/it]
Running inference:   0%|          | 0/3 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A2026-01-16 08:58:50,721 WARNING synthesis text ä½ å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³åˆæˆç³»ç»Ÿã€‚ too short than prompt text You are a helpful assistant.<|endofprompt|>å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚, this may lead to bad performance
2026-01-16 08:58:51,234 INFO synthesis text ä½ å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³åˆæˆç³»ç»Ÿã€‚
WARNING 01-16 08:58:51 [preprocess.py:60] Using None for EOS token id because tokenizer is not initialized
2026-01-16 08:58:51,618 INFO yield speech len 1.36, rtf 0.2829970682368559
2026-01-16 08:58:51,738 INFO yield speech len 1.52, rtf 0.0788691796754536

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.02s/it]
Running inference:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:02,  1.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A2026-01-16 08:58:52,260 INFO synthesis text æ”¶åˆ°å¥½å‹ä»è¿œæ–¹å¯„æ¥çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œé‚£ä»½æ„å¤–çš„æƒŠå–œä¸æ·±æ·±çš„ç¥ç¦è®©æˆ‘å¿ƒä¸­å……æ»¡äº†ç”œèœœçš„å¿«ä¹ï¼Œç¬‘å®¹å¦‚èŠ±å„¿èˆ¬ç»½æ”¾ã€‚
WARNING 01-16 08:58:52 [preprocess.py:60] Using None for EOS token id because tokenizer is not initialized
2026-01-16 08:58:52,626 INFO yield speech len 1.36, rtf 0.2690318752737606
2026-01-16 08:58:52,957 INFO yield speech len 1.0, rtf 0.331068754196167
2026-01-16 08:58:53,348 INFO yield speech len 1.0, rtf 0.3908519744873047
2026-01-16 08:58:53,592 INFO yield speech len 1.0, rtf 0.24382710456848145
2026-01-16 08:58:53,900 INFO yield speech len 1.0, rtf 0.30786967277526855
2026-01-16 08:58:54,191 INFO yield speech len 1.0, rtf 0.291057825088501
2026-01-16 08:58:54,492 INFO yield speech len 1.0, rtf 0.30039143562316895
2026-01-16 08:58:54,790 INFO yield speech len 1.0, rtf 0.29835081100463867
2026-01-16 08:58:55,190 INFO yield speech len 1.0, rtf 0.40018391609191895
2026-01-16 08:58:55,503 INFO yield speech len 1.0, rtf 0.31219053268432617
2026-01-16 08:58:55,990 INFO yield speech len 1.08, rtf 0.4515367525595205

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.25s/it]
Running inference:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A2026-01-16 08:58:56,541 INFO synthesis text å…«ç™¾æ ‡å…µå¥”åŒ—å¡ï¼ŒåŒ—å¡ç‚®å…µå¹¶æ’è·‘ï¼Œç‚®å…µæ€•æŠŠæ ‡å…µç¢°ï¼Œæ ‡å…µæ€•ç¢°ç‚®å…µç‚®ã€‚è¿™æ˜¯ä¸€æ®µç»•å£ä»¤ï¼Œç”¨æ¥æµ‹è¯•è¯­éŸ³åˆæˆç³»ç»Ÿå¯¹å¤æ‚æ–‡æœ¬çš„å¤„ç†èƒ½åŠ›ã€‚
WARNING 01-16 08:58:56 [preprocess.py:60] Using None for EOS token id because tokenizer is not initialized
2026-01-16 08:58:56,916 INFO yield speech len 1.36, rtf 0.27567849439733166
2026-01-16 08:58:57,223 INFO yield speech len 1.0, rtf 0.3063790798187256
2026-01-16 08:58:57,531 INFO yield speech len 1.0, rtf 0.30753111839294434
2026-01-16 08:58:57,825 INFO yield speech len 1.0, rtf 0.2943413257598877
2026-01-16 08:58:58,089 INFO yield speech len 1.0, rtf 0.2634866237640381
2026-01-16 08:58:58,294 INFO yield speech len 1.0, rtf 0.20499491691589355
2026-01-16 08:58:58,578 INFO yield speech len 1.0, rtf 0.28443145751953125
2026-01-16 08:58:58,819 INFO yield speech len 1.0, rtf 0.24044442176818848
2026-01-16 08:58:59,180 INFO yield speech len 1.0, rtf 0.3609659671783447
2026-01-16 08:58:59,477 INFO yield speech len 1.0, rtf 0.2964606285095215
2026-01-16 08:58:59,894 INFO yield speech len 1.0, rtf 0.41762661933898926
2026-01-16 08:59:00,292 INFO yield speech len 1.0, rtf 0.3978879451751709
2026-01-16 08:59:00,796 INFO yield speech len 1.0, rtf 0.5035858154296875
2026-01-16 08:59:01,196 INFO yield speech len 1.0, rtf 0.3999900817871094
2026-01-16 08:59:01,693 INFO yield speech len 1.24, rtf 0.40082431608630764

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.70s/it]
Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  4.19s/it]Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.66s/it]

ğŸ“Š Results:
   Mean TTFB: 0.9030s (903.0ms)
   Mean RTF:  0.3637
   Memory:    649MB allocated, 746MB reserved
   GPU Util:  98.0%
[rank0]:[W116 08:59:01.557972477 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

================================================================================
Testing: vLLM FP16 (previous best)
  Quantization: fp16
  vLLM: True
================================================================================
Loading model...
2026-01-16 08:59:08,227 INFO input frame rate=25
/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
[1;31m2026-01-16 08:59:10.918198006 [E:onnxruntime:Default, provider_bridge_ort.cc:1744 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.11: cannot open shared object file: No such file or directory
[m
[0;93m2026-01-16 08:59:10.918224686 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:870 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.[m
2026-01-16 08:59:12,179 INFO use wetext frontend
INFO 01-16 08:59:13 [model.py:547] Resolved architecture: Qwen2ForCausalLM
INFO 01-16 08:59:13 [model.py:1510] Using max model len 32768
WARNING 01-16 08:59:13 [arg_utils.py:1554] --enable-prompt-embeds and --enable-prefix-caching are not supported together in V1. Prefix caching has been disabled.
INFO 01-16 08:59:13 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 01-16 08:59:16 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:21 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:21 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='pretrained_models/Fun-CosyVoice3-0.5B/vllm', speculative_config=None, tokenizer='pretrained_models/Fun-CosyVoice3-0.5B/vllm', skip_tokenizer_init=True, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=pretrained_models/Fun-CosyVoice3-0.5B/vllm, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":256,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:21 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=25889)[0;0m WARNING 01-16 08:59:22 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:22 [gpu_model_runner.py:2602] Starting to load model pretrained_models/Fun-CosyVoice3-0.5B/vllm...
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:22 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:22 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=25889)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=25889)[0;0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.47it/s]
[1;36m(EngineCore_DP0 pid=25889)[0;0m 
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:22 [default_loader.py:267] Loading weights took 0.12 seconds
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:23 [gpu_model_runner.py:2653] Model loading took 0.6953 GiB and 0.276832 seconds
[1;36m(EngineCore_DP0 pid=25889)[0;0m 2026-01-16 08:59:23,463 DEBUG Starting new HTTPS connection (1): stats.vllm.ai:443
[1;36m(EngineCore_DP0 pid=25889)[0;0m 2026-01-16 08:59:23,623 DEBUG https://stats.vllm.ai:443 "POST / HTTP/1.1" 200 None
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:26 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/8801fce406/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:26 [backends.py:559] Dynamo bytecode transform time: 2.84 s
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:27 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.932 s
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:27 [monitor.py:34] torch.compile takes 2.84 s in total
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:28 [gpu_worker.py:298] Available KV cache memory: 5.42 GiB
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:28 [kv_cache_utils.py:1087] GPU KV cache size: 473,472 tokens
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:28 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 14.45x
[1;36m(EngineCore_DP0 pid=25889)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–ˆ        | 7/35 [00:00<00:00, 62.42it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:00, 62.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22/35 [00:00<00:00, 67.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:00<00:00, 68.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 62.57it/s]
[1;36m(EngineCore_DP0 pid=25889)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆ         | 2/19 [00:00<00:00, 18.46it/s]Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:00<00:00, 46.98it/s]Capturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:00<00:00, 56.92it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 53.57it/s]
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:30 [gpu_model_runner.py:3480] Graph capturing finished in 1 secs, took 0.32 GiB
[1;36m(EngineCore_DP0 pid=25889)[0;0m INFO 01-16 08:59:30 [core.py:210] init engine (profile, create kv cache, warmup model) took 7.17 seconds
INFO 01-16 08:59:30 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 29592
2026-01-16 08:59:30,713 WARNING DiT tensorRT fp16 engine have some performance issue, use at caution!
[01/16/2026-08:59:32] [TRT] [I] Loaded engine size: 639 MiB
[01/16/2026-08:59:32] [TRT] [I] [MS] Running engine with multi stream info
[01/16/2026-08:59:32] [TRT] [I] [MS] Number of aux streams is 1
[01/16/2026-08:59:32] [TRT] [I] [MS] Number of total worker streams is 2
[01/16/2026-08:59:32] [TRT] [I] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[01/16/2026-08:59:32] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +165, now: CPU 0, GPU 796 (MiB)
âœ… Model loaded in 26.79s
ğŸ“Š Memory: Allocated 729MB, Reserved 3562MB
Warming up...
  0%|          | 0/1 [00:00<?, ?it/s]2026-01-16 08:59:32,178 WARNING synthesis text ä½ å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³åˆæˆç³»ç»Ÿã€‚ too short than prompt text You are a helpful assistant.<|endofprompt|>å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚, this may lead to bad performance
2026-01-16 08:59:32,661 INFO synthesis text ä½ å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³åˆæˆç³»ç»Ÿã€‚
WARNING 01-16 08:59:32 [preprocess.py:60] Using None for EOS token id because tokenizer is not initialized
2026-01-16 08:59:33,115 INFO yield speech len 1.36, rtf 0.33390802495619826
2026-01-16 08:59:33,208 INFO yield speech len 1.72, rtf 0.05383297454479129
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.05s/it]
Running inference:   0%|          | 0/3 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A2026-01-16 08:59:33,233 WARNING synthesis text ä½ å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³åˆæˆç³»ç»Ÿã€‚ too short than prompt text You are a helpful assistant.<|endofprompt|>å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚, this may lead to bad performance
2026-01-16 08:59:33,711 INFO synthesis text ä½ å¥½ï¼Œæˆ‘æ˜¯è¯­éŸ³åˆæˆç³»ç»Ÿã€‚
WARNING 01-16 08:59:33 [preprocess.py:60] Using None for EOS token id because tokenizer is not initialized
2026-01-16 08:59:34,018 INFO yield speech len 1.36, rtf 0.2257564488579245
2026-01-16 08:59:34,124 INFO yield speech len 1.52, rtf 0.0690273548427381

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.12it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.12it/s]
Running inference:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.12it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A2026-01-16 08:59:34,634 INFO synthesis text æ”¶åˆ°å¥½å‹ä»è¿œæ–¹å¯„æ¥çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œé‚£ä»½æ„å¤–çš„æƒŠå–œä¸æ·±æ·±çš„ç¥ç¦è®©æˆ‘å¿ƒä¸­å……æ»¡äº†ç”œèœœçš„å¿«ä¹ï¼Œç¬‘å®¹å¦‚èŠ±å„¿èˆ¬ç»½æ”¾ã€‚
WARNING 01-16 08:59:34 [preprocess.py:60] Using None for EOS token id because tokenizer is not initialized
2026-01-16 08:59:35,042 INFO yield speech len 1.36, rtf 0.299764030119952
2026-01-16 08:59:35,361 INFO yield speech len 1.0, rtf 0.3191087245941162
2026-01-16 08:59:35,660 INFO yield speech len 1.0, rtf 0.29914021492004395
2026-01-16 08:59:35,913 INFO yield speech len 1.0, rtf 0.2521967887878418
2026-01-16 08:59:36,131 INFO yield speech len 1.0, rtf 0.21771693229675293
2026-01-16 08:59:36,335 INFO yield speech len 1.0, rtf 0.20374131202697754
2026-01-16 08:59:36,541 INFO yield speech len 1.0, rtf 0.205885648727417
2026-01-16 08:59:36,750 INFO yield speech len 1.0, rtf 0.2087550163269043
2026-01-16 08:59:37,023 INFO yield speech len 1.0, rtf 0.27345895767211914
2026-01-16 08:59:37,331 INFO yield speech len 1.0, rtf 0.3076307773590088
2026-01-16 08:59:37,527 INFO yield speech len 1.08, rtf 0.18091621222319426

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.40s/it]
Running inference:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A2026-01-16 08:59:38,039 INFO synthesis text å…«ç™¾æ ‡å…µå¥”åŒ—å¡ï¼ŒåŒ—å¡ç‚®å…µå¹¶æ’è·‘ï¼Œç‚®å…µæ€•æŠŠæ ‡å…µç¢°ï¼Œæ ‡å…µæ€•ç¢°ç‚®å…µç‚®ã€‚è¿™æ˜¯ä¸€æ®µç»•å£ä»¤ï¼Œç”¨æ¥æµ‹è¯•è¯­éŸ³åˆæˆç³»ç»Ÿå¯¹å¤æ‚æ–‡æœ¬çš„å¤„ç†èƒ½åŠ›ã€‚
WARNING 01-16 08:59:38 [preprocess.py:60] Using None for EOS token id because tokenizer is not initialized
2026-01-16 08:59:38,418 INFO yield speech len 1.36, rtf 0.27876952115227194
2026-01-16 08:59:38,717 INFO yield speech len 1.0, rtf 0.29902005195617676
2026-01-16 08:59:38,943 INFO yield speech len 1.0, rtf 0.22541117668151855
2026-01-16 08:59:39,226 INFO yield speech len 1.0, rtf 0.28311800956726074
2026-01-16 08:59:39,505 INFO yield speech len 1.0, rtf 0.2785191535949707
2026-01-16 08:59:39,690 INFO yield speech len 1.0, rtf 0.1853773593902588
2026-01-16 08:59:39,891 INFO yield speech len 1.0, rtf 0.20031952857971191
2026-01-16 08:59:40,091 INFO yield speech len 1.0, rtf 0.19982004165649414
2026-01-16 08:59:40,293 INFO yield speech len 1.0, rtf 0.20189523696899414
2026-01-16 08:59:40,493 INFO yield speech len 1.0, rtf 0.20046114921569824
2026-01-16 08:59:40,736 INFO yield speech len 1.0, rtf 0.24273133277893066
2026-01-16 08:59:41,018 INFO yield speech len 1.0, rtf 0.28177547454833984
2026-01-16 08:59:41,238 INFO yield speech len 1.0, rtf 0.22009706497192383
2026-01-16 08:59:41,518 INFO yield speech len 1.0, rtf 0.2798020839691162
2026-01-16 08:59:41,721 INFO yield speech len 1.24, rtf 0.16327346524884623

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.19s/it]
Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  3.20s/it]Running inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.83s/it]

ğŸ“Š Results:
   Mean TTFB: 0.8648s (864.8ms)
   Mean RTF:  0.2923
   Memory:    716MB allocated, 862MB reserved
   GPU Util:  10.0%
[rank0]:[W116 08:59:41.559916226 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

================================================================================
Testing: vLLM FP16 + Large Batch
  Quantization: fp16
  vLLM: True
  Additional args: {'max_num_batched_tokens': 4096}
================================================================================
Loading model...
âŒ Configuration failed: CosyVoice3.__init__() got an unexpected keyword argument 'max_num_batched_tokens'
Traceback (most recent call last):
  File "/workspace/CosyVoice-tensrortllm/benchmark_advanced.py", line 119, in test_configuration
    model = AutoModel(**kwargs)
            ^^^^^^^^^^^^^^^^^^^
  File "/workspace/CosyVoice-tensrortllm/cosyvoice/cli/cosyvoice.py", line 238, in AutoModel
    return CosyVoice3(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
TypeError: CosyVoice3.__init__() got an unexpected keyword argument 'max_num_batched_tokens'

================================================================================
Testing: vLLM FP16 + XL Batch
  Quantization: fp16
  vLLM: True
  Additional args: {'max_num_batched_tokens': 8192}
================================================================================
Loading model...
âŒ Configuration failed: CosyVoice3.__init__() got an unexpected keyword argument 'max_num_batched_tokens'
Traceback (most recent call last):
  File "/workspace/CosyVoice-tensrortllm/benchmark_advanced.py", line 119, in test_configuration
    model = AutoModel(**kwargs)
            ^^^^^^^^^^^^^^^^^^^
  File "/workspace/CosyVoice-tensrortllm/cosyvoice/cli/cosyvoice.py", line 238, in AutoModel
    return CosyVoice3(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^
TypeError: CosyVoice3.__init__() got an unexpected keyword argument 'max_num_batched_tokens'


====================================================================================================
ğŸ“Š ADVANCED CONFIGURATION COMPARISON
====================================================================================================

Configuration                       TTFB (ms)    RTF        Memory (MB)     Status
----------------------------------------------------------------------------------------------------
vLLM FP32 (baseline)                903.0        0.3637     746             âœ…
vLLM FP16 (previous best)           864.8        0.2923     862             âœ…
vLLM FP16 + Large Batch             N/A          N/A        N/A             âŒ
vLLM FP16 + XL Batch                N/A          N/A        N/A             âŒ

ğŸ† Best Configurations:
   Lowest TTFB:  vLLM FP16 (previous best) (864.8ms)
   Lowest RTF:   vLLM FP16 (previous best) (0.2923)
   Most Memory Efficient: vLLM FP32 (baseline) (746MB)
   Highest Throughput: vLLM FP16 (previous best) (3.42x real-time)

ğŸ’¾ Memory Utilization:
   Total GPU VRAM: 32,607 MB (RTX 5090)
   vLLM FP32 (baseline)                   746MB (  2.3%)
   vLLM FP16 (previous best)              862MB (  2.6%)

âš¡ Speedup vs Baseline (vLLM FP32 (baseline)):
   vLLM FP16 (previous best)           RTF:  1.24x, TTFB:   +4.2%

ğŸ’¾ Results saved to: advanced_benchmark_results.json

âœ… Advanced benchmark complete!
