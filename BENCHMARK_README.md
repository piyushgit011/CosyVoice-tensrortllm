# CosyVoice3 Streaming TTS Benchmark Suite

Complete benchmarking suite for CosyVoice3 0.5B model with vLLM acceleration on RTX 5090.

## üéØ Quick Results

**Best Configuration:** vLLM + FP16
- **TTFB:** 891ms (sub-1 second!)
- **RTF:** 0.31 (3.2x faster than real-time)
- **Max Concurrent Users:** 6-8 (for interactive apps)
- **Cost:** $0.05 per 1M characters (80x cheaper than cloud APIs)

## üìÅ Files Overview

### Benchmark Scripts
- `benchmark_streaming.py` - Test streaming TTS with multiple concurrency levels
- `benchmark_quantized.py` - Compare FP32 vs FP16 quantization performance
- `generate_summary.py` - Generate quick visual summary of results

### Results & Reports
- **`BENCHMARK_REPORT.md`** ‚≠ê - **Comprehensive analysis with recommendations**
- `benchmark_results.json` - Full streaming metrics (195 test cases)
- `quantization_results.json` - Quantization comparison data
- `benchmark_streaming.log` - Detailed execution logs
- `benchmark_quantized.log` - Quantization test logs

### Supporting Files
- `download.py` - Download CosyVoice3 model from HuggingFace/ModelScope
- `README.md` - Original CosyVoice repository README

## üöÄ Quick Start

### 1. Download Model (Already Done)
```bash
python download.py
# Model saved to: pretrained_models/Fun-CosyVoice3-0.5B/
```

### 2. Run Benchmarks

#### Streaming Performance Test
```bash
python benchmark_streaming.py
# Tests: Concurrency 1, 2, 4, 8, 16
# Duration: ~10 minutes
# Output: benchmark_results.json
```

#### Quantization Comparison
```bash
python benchmark_quantized.py
# Tests: vLLM FP32/FP16, PyTorch FP32/FP16
# Duration: ~5 minutes
# Output: quantization_results.json
```

### 3. View Results
```bash
python generate_summary.py
# Displays quick visual summary

# Or read comprehensive report:
cat BENCHMARK_REPORT.md
```

## üìä Key Metrics Explained

### TTFB (Time To First Byte)
- Time until first audio chunk is generated
- **Critical for streaming applications**
- Target: < 1s for interactive apps, < 2s for general use

### RTF (Real-Time Factor)
- Ratio of generation time to audio duration
- RTF < 1.0 means faster than real-time
- **Target:** < 0.5 for smooth streaming, < 1.0 for interactive

### Throughput
- Characters processed per second
- **Aggregate throughput:** Total system capacity
- Higher is better for batch processing

## üéØ Recommended Configurations

### Interactive Chatbot / Voice Assistant
```python
config = {
    'load_vllm': True,
    'fp16': True,
    'max_concurrent_users': 4,
}
# Expected: TTFB ~1.1s, RTF 0.5
```

### Video Dubbing / Content Creation
```python
config = {
    'load_vllm': True,
    'fp16': True,
    'max_concurrent_users': 8,
}
# Expected: TTFB ~2s, RTF 1.2
```

### Audiobook / Batch Processing
```python
config = {
    'load_vllm': True,
    'fp16': True,
    'max_concurrent_users': 16,
}
# Expected: Throughput 42+ chars/s
```

## üîß Hardware Requirements

### Minimum
- **GPU:** NVIDIA RTX 3080 (10GB VRAM)
- **RAM:** 16GB system memory
- **Storage:** 10GB for model + dependencies

### Recommended (This Benchmark)
- **GPU:** NVIDIA RTX 5090 (32GB VRAM)
- **RAM:** 32GB system memory
- **Storage:** 20GB (with logs)

### Optimal Production
- **GPU:** NVIDIA A100 40GB or H100 80GB
- **RAM:** 64GB+ system memory
- **Multiple GPUs:** For scaling beyond 20 concurrent users

## üìà Performance by Concurrency

| Users | TTFB    | RTF  | Use Case                    |
|-------|---------|------|-----------------------------|
| 1     | 0.89s   | 0.43 | ‚úÖ Real-time demos          |
| 2-4   | 1.1-1.4s| 0.5-0.7 | ‚úÖ Interactive apps      |
| 6-8   | ~2.0s   | 1.2  | ‚ö†Ô∏è Batch processing         |
| 16+   | 3.5s+   | 2.3+ | ‚ùå Non-real-time only       |

## üí° Optimization Tips

### 1. Use vLLM + FP16
```bash
# 76% speedup over PyTorch FP32
# 20% better than vLLM FP32
pip install vllm==0.11.0
```

### 2. Enable Prefix Caching (Future)
```python
# Not yet tested, could improve TTFB by 20-30%
# For applications with repeated prompts
enable_prefix_caching=True
```

### 3. Tune Concurrency
```python
# Find sweet spot for your use case
# Monitor P95 TTFB and RTF
max_concurrent = find_optimal_concurrency()
```

### 4. Load Balancing
```python
# For > 16 users, use multiple GPUs
# Or queue with intelligent routing
if concurrent_users > 8:
    route_to_second_gpu()
```

## üêõ Troubleshooting

### Model Loading Issues
```bash
# If model fails to load:
git submodule update --init --recursive
pip install -r requirements.txt
```

### OOM Errors
```bash
# Reduce concurrency or use FP16
# Check VRAM usage:
nvidia-smi
```

### Slow Performance
```bash
# Ensure TensorRT is working:
# Should see: "Using TensorRT for flow decoder"
# Check logs for warnings
```

## üìù Interpreting Results

### Good Performance Indicators
‚úÖ TTFB < 1.5s for single user
‚úÖ RTF < 1.0 for target concurrency
‚úÖ Success rate = 100%
‚úÖ P95 metrics within 50% of mean

### Warning Signs
‚ö†Ô∏è TTFB increasing super-linearly with concurrency
‚ö†Ô∏è RTF > 2.0 at low concurrency
‚ö†Ô∏è High variance in metrics (unstable)
‚ö†Ô∏è KV cache usage > 90%

### Critical Issues
‚ùå Any failed requests
‚ùå OOM errors
‚ùå TTFB > 5s
‚ùå RTF > 3.0

## üî¨ Benchmark Methodology

### Test Sentences
- **7 diverse sentences** (12-55 characters)
- Short, medium, and long texts
- Chinese language (model's primary language)
- Tongue twisters included for complexity

### Metrics Collection
- **TTFB:** Time to first audio chunk
- **RTF:** Total generation time / audio duration
- **Latency:** End-to-end request time
- **Throughput:** Characters per second
- **Success rate:** Percentage of successful requests

### Statistical Analysis
- **Mean:** Average performance
- **Median:** Typical user experience
- **P95:** Worst-case for 95% of users
- **Min/Max:** Best/worst observed

## üåê Use in Production

### Deployment Checklist
- [ ] Choose configuration based on use case (see recommendations)
- [ ] Set appropriate max_concurrent_users limit
- [ ] Implement request queuing/rate limiting
- [ ] Monitor P95 TTFB and RTF in production
- [ ] Set up alerts for degraded performance
- [ ] Plan for scaling (multiple GPUs if needed)
- [ ] Test with your actual use case workload

### Monitoring
```python
# Track these metrics:
metrics = {
    'p95_ttfb': max_acceptable_ttfb,
    'p95_rtf': max_acceptable_rtf,
    'kv_cache_usage': 0.8,  # 80% max
    'success_rate': 0.99,   # 99% min
}
```

## üìö Additional Resources

- **Original CosyVoice Repo:** https://github.com/FunAudioLLM/CosyVoice
- **Paper:** https://arxiv.org/pdf/2505.17589
- **Model Card:** https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512
- **vLLM Docs:** https://docs.vllm.ai/

## ü§ù Contributing

To add more benchmark scenarios:

1. Copy `benchmark_streaming.py`
2. Modify test cases (texts, concurrency levels)
3. Run and compare results
4. Share findings!

## üìÑ License

Same as CosyVoice repository (Apache 2.0)

## ‚ú® Summary

**This benchmark demonstrates that CosyVoice3 0.5B with vLLM on RTX 5090 is production-ready for:**
- ‚úÖ Interactive voice assistants (4-6 concurrent users)
- ‚úÖ Real-time content generation (sub-1s latency)
- ‚úÖ Batch audiobook/podcast creation (42+ chars/s)
- ‚úÖ Cost-effective alternative to cloud TTS (80x cheaper)

**Not recommended for:**
- ‚ùå Very high concurrency without multiple GPUs (> 16 users)
- ‚ùå Ultra-low latency requirements (< 500ms TTFB)
- ‚ùå Applications requiring guaranteed < 1s P99 at high load

---

**Questions?** Check `BENCHMARK_REPORT.md` for comprehensive analysis!
